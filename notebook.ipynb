{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Capstone Project\n",
    "### Author: Tung Pham Duc\n",
    "\n",
    "#### Project Summary\n",
    "This project build up a data warehouse by integrating immigration data and demography data together to provide a wider range single-source-of-truth database.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "* With the final solution, I can answer questions such as \"People from what age of which country in which season, tend to move to the US?\" or \"How often people from Vietnam move to US yearly?\". The output can be used for both machine learning or ordinary analysis.\n",
    "\n",
    "* To do this, I'll take data from 3 reliable sources, design ETL Pipelines with test scripts, document the configurable params to open the options to run on both Local and Cloud environment. Using Cloud env will help the scalability of the project.\n",
    "\n",
    "#### Data\n",
    "\n",
    "* The Climate Change: Earth Surface Temprature Data was taken from [Kaggle](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data), provided in CSV format. I chose to explore the GlobalLandTempraturesByState.csv since it's closely related to i94addr.\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| Country | country |\n",
    "| State | state |\n",
    "| dt | recorded date |\n",
    "| AverageTemperature | calculated average temperature |\n",
    "\n",
    "* The US Cities Demographics Data comes from OpenSoft, contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65000. It is provided in CSV format. Here attribute names pretty much self-explained its purpose.\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| City |  |\n",
    "| State Code |  |\n",
    "| State |  |\n",
    "| Median Age |  |\n",
    "| Male Population |  |\n",
    "| Female Population |  |\n",
    "| Number of Veterans |  |\n",
    "| Foreign-born |  |\n",
    "| Average Household Size |  |\n",
    "\n",
    "* The I94 Immigration Data comes from the US National Tourism and Trade Office. It is provided in SAS7BDAT format which is a binary database storage format.\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| i94yr | year of migration |\n",
    "| i94mon | month of migration |\n",
    "| i94cit | original country which migrant travels from |\n",
    "| i94port | original port which migrant travels from |\n",
    "| i94mode | travel method |\n",
    "| i94addr | destination State |\n",
    "| biryear | migrant year of birth |\n",
    "| i94visa | VISA type |\n",
    "| visatype | class of admission legally admitting the non-immigrant to temporarily stay in USA |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/19 14:43:05 WARN Utils: Your hostname, rbtwmachine-pc resolves to a loopback address: 127.0.1.1; using 172.31.117.26 instead (on interface eth0)\n",
      "22/06/19 14:43:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/19 14:43:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from os import environ, getcwd\n",
    "from os.path import dirname\n",
    "from sys import path\n",
    "\n",
    "path.append(dirname(getcwd()))\n",
    "\n",
    "from udacity_capstone.pipelines.pipeline_dim_temperatures import DimTemperaturesPipeline\n",
    "from udacity_capstone.pipelines.pipeline_dim_temperatures_test import DimTemperaturesTestPipeline\n",
    "from udacity_capstone.pipelines.pipeline_dim_demographics import DimDemographicsPipeline\n",
    "from udacity_capstone.pipelines.pipeline_dim_demographics_test import DimDemographicsTestPipeline\n",
    "from udacity_capstone.pipelines.pipeline_dim_immigrations import DimImmigrationsPipeline\n",
    "from udacity_capstone.pipelines.pipeline_dim_immigrations_test import DimImmigrationsTestPipeline\n",
    "from udacity_capstone.pipelines.pipeline_fact_immigrations import FactImmigrationsPipeline\n",
    "from udacity_capstone.pipelines.pipeline_fact_immigrations_test import FactImmigrationsTestPipeline\n",
    "\n",
    "from udacity_capstone.pipelines import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "\n",
    "dim_temperatures_pipeline = DimTemperaturesPipeline(\n",
    "    f\"{environ['INPUT_DIR']}/GlobalLandTemperaturesByState.csv\"\n",
    ")\n",
    "dim_demographics_pipeline = DimDemographicsPipeline(\n",
    "    f\"{environ['INPUT_DIR']}/us-cities-demographics.csv\"\n",
    ")\n",
    "dim_immigrations_pipeline = DimImmigrationsPipeline(\n",
    "    f\"{environ['INPUT_DIR']}/immigration_data_sample.csv\"\n",
    ")\n",
    "fact_immigrations_pipeline = FactImmigrationsPipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore, Assess & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Climate Change: Earth Surface Temprature By State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we only play with USA, let's filter the Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "|                 dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "|1743-11-01 00:00:00|10.722000000000001|                        2.898|Alabama|United States|\n",
      "|1744-04-01 00:00:00|            19.075|                        2.902|Alabama|United States|\n",
      "|1744-05-01 00:00:00|            21.197|                        2.844|Alabama|United States|\n",
      "|1744-06-01 00:00:00|             25.29|                        2.879|Alabama|United States|\n",
      "|1744-07-01 00:00:00|             26.42|                        2.841|Alabama|United States|\n",
      "|1744-09-01 00:00:00|            21.735|                        2.866|Alabama|United States|\n",
      "|1744-10-01 00:00:00|             15.63|                        2.872|Alabama|United States|\n",
      "|1744-11-01 00:00:00|            11.198|                        2.806|Alabama|United States|\n",
      "|1744-12-01 00:00:00|             7.063|                        2.823|Alabama|United States|\n",
      "|1745-01-01 00:00:00|             6.931|                        2.838|Alabama|United States|\n",
      "+-------------------+------------------+-----------------------------+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperatures_pipeline.extract()\n",
    "dim_temperatures_pipeline.df.printSchema()\n",
    "dim_temperatures_pipeline.df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: US Cities Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: double (nullable = true)\n",
      " |-- Male Population: integer (nullable = true)\n",
      " |-- Female Population: integer (nullable = true)\n",
      " |-- Total Population: integer (nullable = true)\n",
      " |-- Number of Veterans: integer (nullable = true)\n",
      " |-- Foreign-born: integer (nullable = true)\n",
      " |-- Average Household Size: double (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|         State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|      Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy| Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|       Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|    California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|    New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "|          Peoria|      Illinois|      33.1|          56229|            62432|          118661|              6634|        7517|                   2.4|        IL|American Indian a...| 1343|\n",
      "|        Avondale|       Arizona|      29.1|          38712|            41971|           80683|              4815|        8355|                  3.18|        AZ|Black or African-...|11592|\n",
      "|     West Covina|    California|      39.8|          51629|            56860|          108489|              3800|       37038|                  3.56|        CA|               Asian|32716|\n",
      "|        O'Fallon|      Missouri|      36.0|          41762|            43270|           85032|              5783|        3269|                  2.77|        MO|  Hispanic or Latino| 2583|\n",
      "|      High Point|North Carolina|      35.5|          51751|            58077|          109828|              5204|       16315|                  2.65|        NC|               Asian|11060|\n",
      "+----------------+--------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics_pipeline.extract()\n",
    "dim_demographics_pipeline.df.printSchema()\n",
    "dim_demographics_pipeline.df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: I94 Immigration from US National Tourism and Trade Office\n",
    "\n",
    "Inside I94_SAS_Labels_Description.SAS, we have list of US States and Visa Types, so firstly I had to export them by hand.\n",
    "Then 2 files us-states.txt and visa.txt will be for mapping purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: integer (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: integer (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "22/06/19 14:43:15 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/06/19 14:43:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, admnum, fltno, visatype\n",
      " Schema: _c0, cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, count, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, admnum, fltno, visatype\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/neot/Development/udacity-dend-capstone/in/immigration_data_sample.csv\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|    _c0|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|2027561|4084316.0|2013.0|   4.0| 209.0| 209.0|    HHW|20566.0|    1.0|     HI|20573.0|  61.0|    2.0|  1.0|20160422|    null| null|      G|      O|   null|      M| 1955.0|07202016|     F|  null|     JL|5.6582674633E10|00782|      WT|\n",
      "|2171295|4422636.0|2013.0|   4.0| 582.0| 582.0|    MCA|20567.0|    1.0|     TX|20568.0|  26.0|    2.0|  1.0|20160423|     MTR| null|      G|      R|   null|      M| 1990.0|10222016|     M|  null|    *GA| 9.436199593E10|XBLNG|      B2|\n",
      "| 589494|1195600.0|2013.0|   4.0| 148.0| 112.0|    OGG|20551.0|    1.0|     FL|20571.0|  76.0|    2.0|  1.0|20160407|    null| null|      G|      O|   null|      M| 1940.0|07052016|     M|  null|     LH|5.5780468433E10|00464|      WT|\n",
      "|2631158|5291768.0|2013.0|   4.0| 297.0| 297.0|    LOS|20572.0|    1.0|     CA|20581.0|  25.0|    2.0|  1.0|20160428|     DOH| null|      G|      O|   null|      M| 1991.0|10272016|     M|  null|     QR| 9.478969603E10|00739|      B2|\n",
      "|3032257| 985523.0|2013.0|   4.0| 111.0| 111.0|    CHM|20550.0|    3.0|     NY|20553.0|  19.0|    2.0|  1.0|20160406|    null| null|      Z|      K|   null|      M| 1997.0|07042016|     F|  null|   null|4.2322572633E10| LAND|      WT|\n",
      "| 721257|1481650.0|2013.0|   4.0| 577.0| 577.0|    ATL|20552.0|    1.0|     GA|20606.0|  51.0|    2.0|  1.0|20160408|    null| null|      T|      N|   null|      M| 1965.0|10072016|     M|  null|     DL|   7.36852585E8|  910|      B2|\n",
      "|1072780|2197173.0|2013.0|   4.0| 245.0| 245.0|    SFR|20556.0|    1.0|     CA|20635.0|  48.0|    2.0|  1.0|20160412|    null| null|      T|      O|   null|      M| 1968.0|10112016|     F|  null|     CX|   7.86312185E8|  870|      B2|\n",
      "| 112205| 232708.0|2013.0|   4.0| 113.0| 135.0|    NYC|20546.0|    1.0|     NY|20554.0|  33.0|    2.0|  1.0|20160402|    null| null|      G|      O|   null|      M| 1983.0|06302016|     F|  null|     BA|5.5474485033E10|00117|      WT|\n",
      "|2577162|5227851.0|2013.0|   4.0| 131.0| 131.0|    CHI|20572.0|    1.0|     IL|20575.0|  39.0|    2.0|  1.0|20160428|    null| null|      O|      O|   null|      M| 1977.0|07262016|  null|  null|     LX|5.9413424733E10|00008|      WT|\n",
      "|  10930|  13213.0|2013.0|   4.0| 116.0| 116.0|    LOS|20545.0|    1.0|     CA|20553.0|  35.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1981.0|06292016|  null|  null|     AA|5.5449792933E10|00109|      WT|\n",
      "+-------+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrations_pipeline.extract()\n",
    "dim_immigrations_pipeline.df.printSchema()\n",
    "dim_immigrations_pipeline.df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "All fields should have data, there is no \"NOT NULL\".\n",
    "\n",
    "##### 3.1.1: I'll take Climate Change data model and transform to this schema\n",
    "```\n",
    "dim_temperatures\n",
    " |-- state_id: string\n",
    " |-- year: integer\n",
    " |-- month: integer\n",
    " |-- avg_tmp: double\n",
    "```\n",
    "\n",
    "##### 3.1.2: I'll take Cities Demographics data model and transform to this schema\n",
    "```\n",
    "dim_demographics\n",
    " |-- id: string\n",
    " |-- name: string\n",
    " |-- male_population: long\n",
    " |-- female_population: long\n",
    " |-- veterans_count: long\n",
    " |-- foreigners_count: long\n",
    " |-- avg_household_size: double\n",
    "```\n",
    "\n",
    "##### 3.1.3: I'll take Immigration data model and transform to this schema\n",
    "```\n",
    "dim_immigrations\n",
    " |-- cicid: integer\n",
    " |-- year: integer\n",
    " |-- month: integer\n",
    " |-- travel_method: string\n",
    " |-- origin_country: integer\n",
    " |-- origin_port: string\n",
    " |-- terminus_state_id: string\n",
    " |-- migrant_yob: integer\n",
    " |-- visa_type: string\n",
    " |-- visa_class: string\n",
    "```\n",
    "\n",
    "##### Finally, I create the fact table using all the data from dimensions above\n",
    "```\n",
    "fact_immigrations\n",
    " |-- cicid: integer\n",
    " |-- year: integer\n",
    " |-- month: integer\n",
    " |-- travel_method: string\n",
    " |-- origin_country: integer\n",
    " |-- terminus_state_id: string\n",
    " |-- terminus_state_name: string\n",
    " |-- terminus_temperature: double\n",
    " |-- terminus_avg_household_size: double\n",
    " |-- terminus_male_population: long\n",
    " |-- terminus_female_population: long\n",
    " |-- visa_type: string\n",
    " |-- visa_class: string\n",
    "```\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "![erd](../erd.jpg)\n",
    "\n",
    "For each of the tables, I created a Pipeline & TestPipeline to do ETL process & test its Transform/Load.\n",
    "```\n",
    "- src/pipelines/pipeline_dim_temperatures.py\n",
    "- src/pipelines/pipeline_dim_temperatures_test.py\n",
    "- src/pipelines/pipeline_dim_demographics.py\n",
    "- src/pipelines/pipeline_dim_demographics_test.py\n",
    "- src/pipelines/pipeline_dim_immigrations.py\n",
    "- src/pipelines/pipeline_dim_immigrations_test.py\n",
    "- src/pipelines/pipeline_fact_immigrations.py\n",
    "- src/pipelines/pipeline_fact_immigrations_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "For each Pipeline, I always have:\n",
    "- 1 function for Extract, which will use PySpark reader to ingest data from input files (could be from local or S3)\n",
    "- 1 function for Transform, which will perform Map / Reduce on output of Extract, I take advantage of PySpark Dataframe\n",
    "- 1 function for Load, which will use PySpark writer to save data (could be local csv files or Redshift database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 dim_temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstly have to extract, build a map of states as documented in I94_SAS_Labels_Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardize column names & doing some aggregations, I do analytics by months, not days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_id: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- avg_tmp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperatures_pipeline.transform()\n",
    "dim_temperatures_pipeline.df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:udacity_capstone.pipelines:saving data...                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+------------------+\n",
      "|state_id|year|month|           avg_tmp|\n",
      "+--------+----+-----+------------------+\n",
      "|      AL|1755|    3|            11.918|\n",
      "|      AL|1766|    4|            17.227|\n",
      "|      AL|1776|    2|             10.31|\n",
      "|      AL|1793|   11|11.392000000000001|\n",
      "|      AL|1800|    7|            26.764|\n",
      "|      AL|1803|    1|             6.121|\n",
      "|      AL|1807|    8|            26.704|\n",
      "|      AL|1808|    5|            20.177|\n",
      "|      AL|1866|    2|             7.459|\n",
      "|      AL|1934|    5|            21.494|\n",
      "+--------+----+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dim_temperatures_pipeline.df.show(10)\n",
    "dim_temperatures_pipeline.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 dim_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardize column names & doing some aggregations, because I dont use City, I do analytics on State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- male_population: long (nullable = true)\n",
      " |-- female_population: long (nullable = true)\n",
      " |-- veterans_count: long (nullable = true)\n",
      " |-- foreigners_count: long (nullable = true)\n",
      " |-- avg_household_size: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics_pipeline.transform()\n",
    "dim_demographics_pipeline.df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:udacity_capstone.pipelines:saving data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+-----------------+--------------+----------------+------------------+\n",
      "| id|                name|male_population|female_population|veterans_count|foreigners_count|avg_household_size|\n",
      "+---+--------------------+---------------+-----------------+--------------+----------------+------------------+\n",
      "| MT|             Montana|         438535|           467935|         69270|           29885|2.2749999999999995|\n",
      "| NC|      North Carolina|        7330525|          7970470|        830730|         1896635|2.4750000000000014|\n",
      "| MD|            Maryland|        3139755|          3420890|        320715|         1148970|             2.655|\n",
      "| CO|            Colorado|        7273095|          7405250|        939480|         1688155|2.5599999999999996|\n",
      "| CT|         Connecticut|        2123435|          2231661|        122546|         1114250|2.6661538461538465|\n",
      "| IL|            Illinois|       10943864|         11570526|        723049|         4632600|2.7318681318681306|\n",
      "| NJ|          New Jersey|        3423033|          3507991|        146632|         2327750|2.9608771929824558|\n",
      "| DE|            Delaware|         163400|           196385|         15315|           16680|              2.45|\n",
      "| DC|District of Columbia|        1598525|          1762615|        129815|          475585|              2.24|\n",
      "| AR|            Arkansas|        1400724|          1482165|        154390|          307753| 2.526896551724138|\n",
      "+---+--------------------+---------------+-----------------+--------------+----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demographics_pipeline.df.show(10)\n",
    "dim_demographics_pipeline.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 dim_immigrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardize column names & select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- travel_method: string (nullable = true)\n",
      " |-- origin_country: integer (nullable = true)\n",
      " |-- origin_port: string (nullable = true)\n",
      " |-- terminus_state_id: string (nullable = true)\n",
      " |-- migrant_yob: integer (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- visa_class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrations_pipeline.transform()\n",
    "dim_immigrations_pipeline.df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:udacity_capstone.pipelines:saving data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+-------------+--------------+-----------+-----------------+-----------+---------+----------+\n",
      "|    cicid|year|month|travel_method|origin_country|origin_port|terminus_state_id|migrant_yob|visa_type|visa_class|\n",
      "+---------+----+-----+-------------+--------------+-----------+-----------------+-----------+---------+----------+\n",
      "|4084316.0|2013|    4|          Air|           209|        HHW|               HI|       1955| Pleasure|        WT|\n",
      "|4422636.0|2013|    4|          Air|           582|        MCA|               TX|       1990| Pleasure|        B2|\n",
      "|1195600.0|2013|    4|          Air|           148|        OGG|               FL|       1940| Pleasure|        WT|\n",
      "|5291768.0|2013|    4|          Air|           297|        LOS|               CA|       1991| Pleasure|        B2|\n",
      "| 985523.0|2013|    4|         Land|           111|        CHM|               NY|       1997| Pleasure|        WT|\n",
      "|1481650.0|2013|    4|          Air|           577|        ATL|               GA|       1965| Pleasure|        B2|\n",
      "|2197173.0|2013|    4|          Air|           245|        SFR|               CA|       1968| Pleasure|        B2|\n",
      "| 232708.0|2013|    4|          Air|           113|        NYC|               NY|       1983| Pleasure|        WT|\n",
      "|5227851.0|2013|    4|          Air|           131|        CHI|               IL|       1977| Pleasure|        WT|\n",
      "|  13213.0|2013|    4|          Air|           116|        LOS|               CA|       1981| Pleasure|        WT|\n",
      "+---------+----+-----+-------------+--------------+-----------+-----------------+-----------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrations_pipeline.df.show(10)\n",
    "dim_immigrations_pipeline.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.4 fact_immigrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- travel_method: string (nullable = true)\n",
      " |-- origin_country: integer (nullable = true)\n",
      " |-- terminus_state_id: string (nullable = true)\n",
      " |-- terminus_state_name: string (nullable = true)\n",
      " |-- terminus_temperature: double (nullable = true)\n",
      " |-- terminus_avg_household_size: double (nullable = true)\n",
      " |-- terminus_male_population: long (nullable = true)\n",
      " |-- terminus_female_population: long (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- visa_class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigrations_pipeline.extract()\n",
    "fact_immigrations_pipeline.transform()\n",
    "fact_immigrations_pipeline.df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:udacity_capstone.pipelines:saving data...                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----+-------------+--------------+-----------------+-------------------+--------------------+---------------------------+------------------------+--------------------------+---------+----------+\n",
      "|    cicid|year|month|travel_method|origin_country|terminus_state_id|terminus_state_name|terminus_temperature|terminus_avg_household_size|terminus_male_population|terminus_female_population|visa_type|visa_class|\n",
      "+---------+----+-----+-------------+--------------+-----------------+-------------------+--------------------+---------------------------+------------------------+--------------------------+---------+----------+\n",
      "|5762766.0|2013|    4|          Air|           343|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "|1214788.0|2013|    4|          Air|           245|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "|3697368.0|2013|    4|          Air|           213|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "| 813390.0|2013|    4|          Air|           104|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Business|        WB|\n",
      "| 636324.0|2013|    4|          Air|           111|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        WT|\n",
      "|1434452.0|2013|    4|          Air|           516|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "|1216269.0|2013|    4|          Air|           245|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "|3669469.0|2013|    4|          Air|           129|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Business|        WB|\n",
      "|2929149.0|2013|    4|          Air|           213|               NC|     North Carolina|              15.002|         2.4750000000000014|                 7330525|                   7970470| Pleasure|        B2|\n",
      "| 873211.0|2013|    4|          Air|           251|               MD|           Maryland|              12.345|                      2.655|                 3139755|                   3420890| Pleasure|        B2|\n",
      "+---------+----+-----+-------------+--------------+-----------------+-------------------+--------------------+---------------------------+------------------------+--------------------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fact_immigrations_pipeline.df.show(10)\n",
    "fact_immigrations_pipeline.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Data Quality is ensured by pipeline tests script.\n",
    "  * there must be no NULL fields\n",
    "  * number of records must not be 0\n",
    "  * records must not be duplicated, this depends on how each tables define its primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:udacity_capstone.pipelines.pipeline_dim_temperatures_test:check passed!                 \n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_temperatures_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_temperatures_test:check passed!                 \n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_demographics_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_demographics_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_demographics_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_demographics_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_demographics_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_immigrations_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_immigrations_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_dim_immigrations_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_fact_immigrations_test:check passed!                \n",
      "INFO:udacity_capstone.pipelines.pipeline_fact_immigrations_test:check passed!\n",
      "INFO:udacity_capstone.pipelines.pipeline_fact_immigrations_test:check passed!                \n"
     ]
    }
   ],
   "source": [
    "DimTemperaturesTestPipeline(dim_temperatures_pipeline).run()\n",
    "\n",
    "DimDemographicsTestPipeline(dim_demographics_pipeline).run()\n",
    "\n",
    "DimImmigrationsTestPipeline(dim_immigrations_pipeline).run()\n",
    "\n",
    "FactImmigrationsTestPipeline(fact_immigrations_pipeline).run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "* dim_temperatures\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| state_id  | state code, documented in us-states.txt |\n",
    "| year      | recored year |\n",
    "| month     | recorded month |\n",
    "| avg_tmp   | calculated average temperature |\n",
    "\n",
    "\n",
    "* dim_demographics\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| id                 | state code, documented in us-states.txt |\n",
    "| name               | state name, documented in us-states.txt |\n",
    "| male_population    | male population |\n",
    "| female_population  | female population |\n",
    "| veterans_count     | veterans (old people) |\n",
    "| foreigners_count   | foreign borns |\n",
    "| avg_household_size | average household size |\n",
    "\n",
    "\n",
    "* dim_immigrations\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| cicid             | unique id from sas file, I94 Org generated it themself |\n",
    "| year              | year of migration |\n",
    "| month             | month of migration |\n",
    "| travel_method     | original country which migrant travels from |\n",
    "| origin_country    | original port which migrant travels from |\n",
    "| origin_port       | travel method |\n",
    "| terminus_state_id | destination state |\n",
    "| migrant_yob       | migrant year of birth |\n",
    "| visa_type         | VISA type |\n",
    "| visa_class        | class of admission legally admitting the non-immigrant to temporarily stay in USA |\n",
    "\n",
    "* fact_immigrations\n",
    "\n",
    "| attribute | desc |\n",
    "|--|--|\n",
    "| cicid                       | unique id from sas file, I94 Org generated it themself |\n",
    "| year                        | year of migration |\n",
    "| month                       | month of migration |\n",
    "| travel_method               | original country which migrant travels from |\n",
    "| origin_country              | original port which migrant travels from |\n",
    "| terminus_state_id           | destination state code |\n",
    "| terminus_state_name         | destination state name, linked from dim_demographics |\n",
    "| terminus_temperature        | destination state average temperature by month, linked from dim_temperatures |\n",
    "| terminus_avg_household_size | destination state average household size, linked from dim_demographics |\n",
    "| terminus_male_population    | destination state male population, linked from dim_demographics |\n",
    "| terminus_female_population  | destination state female population, linked from dim_demographics |\n",
    "| visa_type                   | VISA type |\n",
    "| visa_class                  | class of admission legally admitting the non-immigrant to temporarily stay in USA |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "##### 5.1 Tools / Libraries\n",
    "* **Poetry** \\\n",
    "I feel safer and more confident using a proper package manager rather than the default Pip. Poetry helps installing things much simpler where you can select version interactively. It generates a lockfile, and clearly states version range and dependencies.\n",
    "\n",
    "* **Black & Pylint** for Styling / Linting \\\n",
    "I often protect my code quality by installing formatters and linters. Having these 2, I no longer suffer from manually format my code.\n",
    "\n",
    "* **PySpark** \\\n",
    "To deal with Batch ETL Pipelines, PySpark is a good choice. It has the Spark SQL which is powerful and make me fun to explore and investigate data.\n",
    "\n",
    "* **Amazon S3** (optional, configurable via config.ini) \\\n",
    "A Cloud-based storage service, I can either do ETL on Cloud or Local.\n",
    "\n",
    "* **Amazon Redshift** (optional, configurable via config.ini) \\\n",
    "A Cloud-based database, highly scalable. I can choose to save by CSV files or a relational database like Redshift. For now, it's running on Local Env.\n",
    "\n",
    "* **Amazon EMR** (optional) \\\n",
    "I can always choose a platform to run data pipelines. Amazone EMR will take care of automatic scaling when velocity or volume increases.\n",
    "\n",
    "##### 5.2 How often data should be updated\n",
    "- Data comes from external source, they do not update regularly; Except for I94, they update by month. ***However, it totally depends on how I do analysis.***\n",
    "\n",
    "- If I'm about to answer the question: \"People from what age of which country in which season, tend to move to the US?\" then it's more like a prediction, machine learning thing. Our current data is already sufficient.\n",
    "\n",
    "- If I'm about to answer the question: \"How often people from Vietnam move to US yearly?\" then ok data must be updated at least until the last year.\n",
    "\n",
    "\n",
    "##### 5.3 Corner scenarios\n",
    "* The data was increased by 100x \\\n",
    "I'll take advantage of ***Amazon EMR*** & ***Amazon Redshift***. These 2 tools will help me do data ETL much more efficient than personal computer, but at a cost.\n",
    "\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day \\\n",
    "I'll design an ***Apache Airflow pipeline***, and use ***Amazon Managed Workflows for Apache Airflow*** to schedule a run at 7AM daily.\n",
    "\n",
    "* The database needed to be accessed by 100+ people \\\n",
    "So instead of using **Amazon S3** or local CSV files, I'll use ***Amazon Redshift***. I'll create IAM account for people in my organization, then share them the Redshift connection. The rest work I'll leave to Amazon Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a46490e38668de7b04a1d3d8ebd07c6e6f3a717f282663990004b69d26dfe2e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
